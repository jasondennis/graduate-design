{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "successful-sweet",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from transformers import BertModel, BertTokenizer, BertConfig\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "quiet-scotland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, training on CPU.\n"
     ]
    }
   ],
   "source": [
    "USE_CUDA=torch.cuda.is_available()\n",
    "if(USE_CUDA):\n",
    "    print('Training on GPU.')\n",
    "else:\n",
    "    print('No GPU available, training on CPU.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "liked-capacity",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from transformers import BertModel, BertTokenizer, BertConfig\n",
    "\n",
    "class bert_lstm(nn.Module):\n",
    "    def __init__(self, hidden_dim,output_size,n_layers,bidirectional=True, drop_prob=0.5):\n",
    "        super(bert_lstm, self).__init__()\n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bidirectional = bidirectional\n",
    "\n",
    "        #Bert ----------------重点，bert模型需要嵌入到自定义模型里面\n",
    "        self.bert=BertModel.from_pretrained('/Users/jasondennis/Desktop/chinese_L-12_H-768_A-12')\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(768, hidden_dim, n_layers, batch_first=True,bidirectional=bidirectional)\n",
    "\n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "\n",
    "        # linear and sigmoid layers\n",
    "        if bidirectional:\n",
    "            self.fc = nn.Linear(hidden_dim*2, output_size)\n",
    "        else:\n",
    "            self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "        #self.sig = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        batch_size = x.size(0)\n",
    "        #生成bert字向量\n",
    "        x=self.bert(x)[0]     #bert 字向量\n",
    "\n",
    "        # lstm_out\n",
    "        #x = x.float()\n",
    "        lstm_out, (hidden_last,cn_last) = self.lstm(x, hidden)\n",
    "        #print(lstm_out.shape)   #[32,100,768]\n",
    "        #print(hidden_last.shape)   #[4, 32, 384]\n",
    "        #print(cn_last.shape)    #[4, 32, 384]\n",
    "\n",
    "        #修改 双向的需要单独处理\n",
    "        if self.bidirectional:\n",
    "            #正向最后一层，最后一个时刻\n",
    "            hidden_last_L=hidden_last[-2]\n",
    "            #print(hidden_last_L.shape)  #[32, 384]\n",
    "            #反向最后一层，最后一个时刻\n",
    "            hidden_last_R=hidden_last[-1]\n",
    "            #print(hidden_last_R.shape)   #[32, 384]\n",
    "            #进行拼接\n",
    "            hidden_last_out=torch.cat([hidden_last_L,hidden_last_R],dim=-1)\n",
    "            #print(hidden_last_out.shape,'hidden_last_out')   #[32, 768]\n",
    "        else:\n",
    "            hidden_last_out=hidden_last[-1]   #[32, 384]\n",
    "\n",
    "\n",
    "        # dropout and fully-connected layer\n",
    "        out = self.dropout(hidden_last_out)\n",
    "        #print(out.shape)    #[32,768]\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "\n",
    "        number = 1\n",
    "        if self.bidirectional:\n",
    "            number = 2\n",
    "\n",
    "        if (USE_CUDA):\n",
    "            hidden = (weight.new(self.n_layers*number, batch_size, self.hidden_dim).zero_().float().cuda(),\n",
    "                      weight.new(self.n_layers*number, batch_size, self.hidden_dim).zero_().float().cuda()\n",
    "                      )\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers*number, batch_size, self.hidden_dim).zero_().float(),\n",
    "                      weight.new(self.n_layers*number, batch_size, self.hidden_dim).zero_().float()\n",
    "                      )\n",
    "\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "raising-october",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /Users/jasondennis/Desktop/chinese_L-12_H-768_A-12 were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_lstm(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (lstm): LSTM(768, 384, num_layers=2, batch_first=True, bidirectional=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=768, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "output_size = 1\n",
    "hidden_dim = 384   #768/2\n",
    "n_layers = 2\n",
    "bidirectional = True  #这里为True，为双向LSTM\n",
    "\n",
    "net = bert_lstm(hidden_dim, output_size,n_layers, bidirectional)\n",
    "\n",
    "print(net)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "assisted-junction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7faec97dbdb0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(2020)\n",
    "torch.manual_seed(2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "saved-evans",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('/Users/jasondennis/Desktop/train_data.csv',encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "heard-least",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretreatment(comments):\n",
    "    result_comments=[]\n",
    "    punctuation='。，？！：%&~（）、；“”&|,.?!:%&~();\"\"'\n",
    "    for comment in comments:\n",
    "        comment= ''.join([c for c in comment if c not in punctuation])\n",
    "        comment= ''.join(comment.split())   #\\xa0\n",
    "        result_comments.append(comment)\n",
    "    \n",
    "    return result_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "devoted-hindu",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_comments=pretreatment(list(data['text_a'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "political-flight",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73928"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bulgarian-burden",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['钢铁侠那神一样的吐槽']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_comments[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "professional-apache",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer,BertModel\n",
    "tokenizer = BertTokenizer.from_pretrained('/Users/jasondennis/Desktop/chinese_L-12_H-768_A-12')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "limited-lending",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_comments_id=tokenizer(result_comments,padding=True,truncation=True,max_length=200,return_tensors='pt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "unusual-patient",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 7167, 7188,  ...,    0,    0,    0],\n",
       "        [ 101, 2600, 1377,  ...,    0,    0,    0],\n",
       "        [ 101, 4035, 2014,  ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [ 101, 3241, 5688,  ...,    0,    0,    0],\n",
       "        [ 101, 2523, 3300,  ...,    0,    0,    0],\n",
       "        [ 101, 5629, 3553,  ...,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_comments_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "missing-library",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "noticed-northeast",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=result_comments_id['input_ids']\n",
    "y=torch.from_numpy(data['label'].values).float()\n",
    "\n",
    "X_train,X_test, y_train, y_test =train_test_split(X,y,test_size=0.3,shuffle=True,stratify=y,random_state=2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "particular-proceeding",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51749, 22179)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train),len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "parliamentary-tactics",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([51749, 200])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "global-fields",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid,X_test,y_valid,y_test=train_test_split(X_test,y_test,test_size=0.5,shuffle=True,stratify=y_test,random_state=2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "reserved-withdrawal",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(X_train, y_train)\n",
    "valid_data = TensorDataset(X_valid, y_valid)\n",
    "test_data = TensorDataset(X_test,y_test)\n",
    "\n",
    "# dataloaders\n",
    "batch_size = 64\n",
    "\n",
    "# make sure the SHUFFLE your training data\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size,drop_last=True)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size,drop_last=True)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "metallic-poster",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss and optimization functions\n",
    "lr=2e-5\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "# training params\n",
    "epochs = 10\n",
    "# batch_size=50\n",
    "print_every = 7\n",
    "clip=5 # gradient clipping\n",
    " \n",
    "# move model to GPU, if available\n",
    "if(USE_CUDA):\n",
    "    net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artistic-television",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0345, -0.0819, -0.0456, -0.1099, -0.0755, -0.0330, -0.1172,  0.0218,\n",
      "        -0.0748, -0.0657, -0.0667, -0.0286, -0.0150, -0.0701,  0.0696, -0.0410,\n",
      "         0.0654,  0.0344,  0.0036, -0.0614, -0.0180, -0.0444, -0.0135, -0.0221,\n",
      "         0.0130, -0.0599,  0.0061, -0.1090,  0.0237, -0.0041, -0.0857, -0.0533,\n",
      "        -0.0357, -0.0462, -0.0220,  0.0007,  0.0162,  0.0481, -0.0484,  0.0262,\n",
      "        -0.0499, -0.0486,  0.0088,  0.0595, -0.0791, -0.1440,  0.0057, -0.0915,\n",
      "        -0.0183, -0.0221, -0.0354, -0.1021, -0.0359, -0.1043,  0.0454, -0.0266,\n",
      "        -0.0529,  0.0826, -0.0058, -0.0509, -0.0633, -0.0694,  0.0440,  0.0089],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1.,\n",
      "        1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0.,\n",
      "        1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 1., 1., 1.])\n",
      "tensor([0.1982, 0.3363, 0.2334, 0.1971, 0.2642, 0.1622, 0.2235, 0.1815, 0.1923,\n",
      "        0.2483, 0.1611, 0.2524, 0.1798, 0.1475, 0.1994, 0.1801, 0.2509, 0.2035,\n",
      "        0.1724, 0.3421, 0.2404, 0.1732, 0.1732, 0.2482, 0.2368, 0.2592, 0.1278,\n",
      "        0.3039, 0.1599, 0.2294, 0.1793, 0.3060, 0.2437, 0.2205, 0.2049, 0.2117,\n",
      "        0.2594, 0.2166, 0.2088, 0.1915, 0.2566, 0.1850, 0.2058, 0.2501, 0.1851,\n",
      "        0.2369, 0.2310, 0.2477, 0.2166, 0.1703, 0.2506, 0.1840, 0.2288, 0.1523,\n",
      "        0.2205, 0.1890, 0.1557, 0.2805, 0.1688, 0.0767, 0.1942, 0.2113, 0.2033,\n",
      "        0.2189], grad_fn=<SqueezeBackward0>)\n",
      "tensor([0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1.,\n",
      "        1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 0., 1., 0., 1., 1.])\n",
      "tensor([0.3802, 0.3387, 0.4094, 0.3195, 0.3729, 0.4278, 0.4381, 0.3931, 0.3911,\n",
      "        0.2957, 0.3317, 0.3827, 0.3419, 0.5079, 0.3398, 0.4209, 0.2885, 0.3253,\n",
      "        0.3259, 0.2515, 0.3444, 0.3569, 0.3591, 0.3462, 0.3884, 0.2613, 0.4613,\n",
      "        0.3198, 0.3809, 0.3789, 0.3440, 0.3038, 0.2811, 0.3072, 0.2854, 0.3416,\n",
      "        0.3688, 0.3917, 0.3366, 0.2853, 0.3596, 0.2717, 0.2786, 0.4178, 0.3125,\n",
      "        0.3504, 0.3318, 0.3094, 0.4055, 0.3463, 0.3824, 0.3599, 0.2418, 0.2710,\n",
      "        0.3602, 0.4556, 0.3013, 0.3635, 0.3562, 0.3600, 0.4251, 0.4314, 0.2939,\n",
      "        0.3805], grad_fn=<SqueezeBackward0>)\n",
      "tensor([1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
      "        1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1.,\n",
      "        0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0.])\n",
      "tensor([0.3764, 0.3757, 0.3920, 0.5245, 0.3957, 0.4558, 0.4991, 0.3717, 0.4534,\n",
      "        0.4268, 0.4116, 0.4485, 0.4759, 0.4483, 0.3738, 0.5495, 0.5326, 0.3211,\n",
      "        0.4399, 0.5864, 0.4901, 0.4160, 0.4240, 0.4015, 0.4404, 0.4380, 0.4451,\n",
      "        0.3433, 0.4993, 0.4370, 0.3535, 0.5091, 0.5698, 0.4096, 0.3969, 0.4832,\n",
      "        0.4848, 0.4873, 0.5549, 0.4556, 0.5103, 0.4625, 0.3898, 0.4972, 0.4505,\n",
      "        0.4070, 0.4179, 0.4866, 0.3543, 0.3702, 0.3995, 0.4730, 0.4929, 0.4954,\n",
      "        0.5461, 0.4127, 0.3660, 0.3359, 0.3524, 0.3944, 0.4373, 0.4416, 0.4288,\n",
      "        0.5265], grad_fn=<SqueezeBackward0>)\n",
      "tensor([0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0.,\n",
      "        1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1.,\n",
      "        0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 1., 0., 0., 1., 1., 1.])\n",
      "tensor([0.5854, 0.5823, 0.5435, 0.4400, 0.5090, 0.5995, 0.5444, 0.4812, 0.4214,\n",
      "        0.5804, 0.5688, 0.4552, 0.5029, 0.6006, 0.5044, 0.5011, 0.5802, 0.5765,\n",
      "        0.6437, 0.5350, 0.5883, 0.5026, 0.5126, 0.6170, 0.5791, 0.5493, 0.4935,\n",
      "        0.4850, 0.4776, 0.5132, 0.5939, 0.5326, 0.5129, 0.5459, 0.5152, 0.4381,\n",
      "        0.6368, 0.6678, 0.3831, 0.5499, 0.5356, 0.5391, 0.5853, 0.6324, 0.4879,\n",
      "        0.5998, 0.5435, 0.5588, 0.5472, 0.6442, 0.4774, 0.5118, 0.6168, 0.4702,\n",
      "        0.5766, 0.4559, 0.5056, 0.5224, 0.5859, 0.5460, 0.5826, 0.6145, 0.4967,\n",
      "        0.5189], grad_fn=<SqueezeBackward0>)\n",
      "tensor([1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1.,\n",
      "        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 1., 1., 1., 1., 0.])\n",
      "tensor([0.7222, 0.5726, 0.6140, 0.6480, 0.7488, 0.6039, 0.6382, 0.5811, 0.5095,\n",
      "        0.6520, 0.6757, 0.6054, 0.5811, 0.6343, 0.6564, 0.7293, 0.6506, 0.5776,\n",
      "        0.6184, 0.6622, 0.5465, 0.6039, 0.7461, 0.7367, 0.5481, 0.6017, 0.6613,\n",
      "        0.6614, 0.6171, 0.6301, 0.6198, 0.6645, 0.7977, 0.6616, 0.5762, 0.6219,\n",
      "        0.6827, 0.6531, 0.6145, 0.5331, 0.6312, 0.5713, 0.7653, 0.5320, 0.6777,\n",
      "        0.6287, 0.5886, 0.5004, 0.6330, 0.5426, 0.6410, 0.6779, 0.6375, 0.6436,\n",
      "        0.6188, 0.5803, 0.6172, 0.6414, 0.6292, 0.6284, 0.5803, 0.6517, 0.5800,\n",
      "        0.5963], grad_fn=<SqueezeBackward0>)\n",
      "tensor([0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
      "        0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 1., 1., 1., 1., 0.])\n",
      "tensor([0.5981, 0.6175, 0.6022, 0.7035, 0.7369, 0.7165, 0.6869, 0.8269, 0.8049,\n",
      "        0.7631, 0.7713, 0.6642, 0.7081, 0.7530, 0.8886, 0.7194, 0.7554, 0.6772,\n",
      "        0.7830, 0.7260, 0.7468, 0.6433, 0.7543, 0.7479, 0.7937, 0.6191, 0.7330,\n",
      "        0.7725, 0.7790, 0.6912, 0.7997, 0.7362, 0.8166, 0.7291, 0.7346, 0.6901,\n",
      "        0.7684, 0.8888, 0.7031, 0.7797, 0.8951, 0.6869, 0.8217, 0.6417, 0.7983,\n",
      "        0.6953, 0.7552, 0.7333, 0.7138, 0.7632, 0.6103, 0.6958, 0.7179, 0.7052,\n",
      "        0.7516, 0.6813, 0.6468, 0.6719, 0.7614, 0.7599, 0.7237, 0.8276, 0.6773,\n",
      "        0.8668], grad_fn=<SqueezeBackward0>)\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1.,\n",
      "        1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0.,\n",
      "        0., 1., 1., 1., 1., 1., 0., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "net.train()\n",
    "# train for some number of epochs\n",
    "for e in range(epochs):\n",
    "    # initialize hidden state\n",
    "    h = net.init_hidden(batch_size)\n",
    "    counter = 0\n",
    " \n",
    "    # batch loop\n",
    "    for inputs, labels in train_loader:\n",
    "        counter += 1\n",
    "        \n",
    "        if(USE_CUDA):\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        h = tuple([each.data for each in h])\n",
    "        net.zero_grad()\n",
    "        output= net(inputs, h)\n",
    "        print(output.squeeze())\n",
    "        print(labels.float())\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    " \n",
    "        # loss stats\n",
    "        if counter % print_every == 0:\n",
    "            net.eval()\n",
    "            with torch.no_grad():\n",
    "                val_h = net.init_hidden(batch_size)\n",
    "                val_losses = []\n",
    "                for inputs, labels in valid_loader:\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "                    if(USE_CUDA):\n",
    "                        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "                    output = net(inputs, val_h)\n",
    "                    val_loss = criterion(output.squeeze(), labels.float())\n",
    "\n",
    "                    val_losses.append(val_loss.item())\n",
    " \n",
    "            net.train()\n",
    "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
